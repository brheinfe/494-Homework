{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvVzTiUwoZQ2opEQJJBTLW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brheinfe/494-Homework/blob/main/Homework2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAE 494 Homework 2**\n",
        "**Problem 1**<br>\n",
        "(a) Show that the stationary point (zero gradient) of the function<br><br>\n",
        "$$f(x_1,x_2) = 2x_1^2-4x_1x_2 + 1.5x_2^2 + x_2$$<br>\n",
        "is a saddle (with indefinite Hessian).<br>\n",
        "(b) Find the directions of downslopes away from the saddle. To do this, use Taylor's expansion at the saddle point to show that<br><br>\n",
        "$$f(x_1,x_2) = f(1,1) + (a{\\delta}x_1-b{\\delta}x_2)(c{\\delta}x_1-d{\\delta}x_2)$$<br>\n",
        "with some constants $a,b,c,d$ and ${\\delta}x_i = x_i-1$ for $i = 1,2$. Then the directions of the downslopes are such $({\\delta}x_1,{\\delta}x_2)$ that<br><br>\n",
        "$$ f(x_1,x_2) - f(1,1) = (a{\\delta}x_1-b{\\delta}x_2)(c{\\delta}x_1-d{\\delta}x_2)<0$$<br><br>\n",
        "(a) We will begin by finding the point of zero gradient for this function by evaluating the gradient, and then finding the point where it is equal to zero.<br>\n",
        "${\\nabla}f(x_1,x_2) =\n",
        "\\left[\\begin{array}{cc}\n",
        "\\frac{{\\partial}f}{{\\partial}x_1} \\\\ \\frac{{\\partial}f}{{\\partial}x_2}\n",
        "\\end{array}\\right] =\n",
        "\\left[\\begin{array}{cc}\n",
        "4x_1-4x_2 \\\\ -4x_1 + 3x_2 + 1\n",
        "\\end{array}\\right]$<br>\n",
        "$\\therefore 0 = 4x_1-4x_2 \\rightarrow x_1 = x_2$<br>\n",
        "$\\therefore x_1 = x_2 = 1$<br>\n",
        "Then taking the Hessian:<br><br>\n",
        "$H = \\left[\\begin{array}{cc}\n",
        "\\frac{{\\partial^2}f}{{\\partial}x_1^2} & \\frac{{\\partial^2}f}{{\\partial}x_1{\\partial}x_2}\\\\ \\frac{{\\partial^2}f}{{\\partial}x_2{\\partial}x_1} & \\frac{{\\partial^2}f}{{\\partial}x_2^2}\n",
        "\\end{array}\\right] = \\left[\\begin{array}{cc}\n",
        "4 & -4 \\\\ -4 & 3\n",
        "\\end{array}\\right]$<br>\n",
        "From this, we can see that the Hessian is indefinite, so we know that the point of zero gradient occurs at a saddle.<br><br>\n",
        "(b)\n",
        "To find the direction of the downslopes, we will use Newton's Method around the point $x_k = (1,1)$, where ${\\delta}x = \\left[\\begin{array}{cc}\n",
        "{\\delta}x_1 \\\\ {\\delta}x_2\n",
        "\\end{array}\\right] = \\left[\\begin{array}{cc}\n",
        "x_1-x_{k,1} \\\\ x_2-x_{k,2}\n",
        "\\end{array}\\right]$:<br><br>\n",
        "$f(x_1,x_2) \\approx f(1,1) + {\\nabla}f(x_k){\\delta}x + \\frac{1}{2} {\\delta}x^TH_k{\\delta}x$<br>\n",
        "$f(x_1,x_2) \\approx 0.5 + 0 + \\frac{1}{2}\\left[\\begin{array}{cc}\n",
        "{\\delta}x_1 & {\\delta}x_2\n",
        "\\end{array}\\right]\n",
        "\\left[\\begin{array}{cc}\n",
        "4 & -4 \\\\ -4 & 3\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "{\\delta}x_1 \\\\ {\\delta}x_2\n",
        "\\end{array}\\right]$<br>\n",
        "$f(x_1,x_2) \\approx 0.5 + \\left[\\begin{array}{cc}\n",
        "2{\\delta}x_1-2{\\delta}x_2 & -2{\\delta}x_1+1.5{\\delta}x_2\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "{\\delta}x_1 \\\\ {\\delta}x_2\n",
        "\\end{array}\\right]$<br>\n",
        "$f(x_1,x_2) \\approx 0.5 + 2{\\delta}x_1^2-2{\\delta}x_1{\\delta}x_2-2{\\delta}x_1{\\delta}x_2+1.5{\\delta}x_2^2$<br>\n",
        "$f(x_1,x_2) \\approx 0.5 + 2{\\delta}x_1^2-4{\\delta}x_1{\\delta}x_2+1.5{\\delta}x_2^2$<br>\n",
        "$f(x_1,x_2) \\approx 0.5+(2{\\delta}x_1-3{\\delta}x_2)({\\delta}x_1-0.5{\\delta}x_2)$<br>\n",
        "Then comparing to the equation from above, we can find the coefficients $a,b,c,d$ to be:<br>\n",
        "$a = 2, b = -3, c = 1, d = -0.5$<br>\n",
        "Then we can use the second equation to calculate ${\\delta}x$ values that result in the equation being negative. It's clear to see that since the terms are multiplied, we want one to positive and the other to be negative. It's also quite simple to see that to accomplish this, ${\\delta}x_1$ and ${\\delta}x_2$ must both be positive, or they must both be negative."
      ],
      "metadata": {
        "id": "_numDy9nxzdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2**<br>\n",
        "(a) Find the point in the plane $x_1+2x_2+3x_3 = 1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3 = 1$.<br>\n",
        "(b) Implement the gradient descent and Newton's alogrithm for solving the problem. Attach your codes in the report, along with a short summary of your findings. The summary should include: (1) The initial points tested; (2) corresponding solutions; (3) A log-linear convergence plot.<br><br>\n",
        "\n",
        "(a) We know from knowledge of Linear Algebra that the smallest distance between the plane and the point would be represented by the segment passing through the point and perpendicular to the plane. The normal vector for this plane will be $n = <1,2,3>$. We can normalize the normal vector so we can write an equation for the line that passes through this point as follows:<br><br>\n",
        "$$f(t) = t<0.267,0.5345,0.8018> + <-1,0,1>$$<br>\n",
        "Where $t$ is some distance from the point along this line. If we define another point on the plane $Q = (1,2,-4/3)$, then we can define the vector between $Q$ and $P$ as $v = <-2,-2,7/3>$. Then we can get the distance by projecting this vector onto n:\n",
        "$$ d = n^T{\\cdot}V$$<br>\n",
        "$$ \\therefore d = 0.2679$$<br>\n",
        "Then we can calculate the point on the plane by plugging this value into our line equation, keeping in mind that the distance we plug in should be negative since the distance is measured from the point in the direction of the normal vector.<br><br>\n",
        "$$f(-0.2679) = (-0.2679)<0.267,0.5345,0.8018> + <-1,0,1>$$<br>\n",
        "$$\\boxed{f(-0.2679) = (-1.072,-0.143,0.7852)}$$<br><br>\n",
        "To formulate this problem for numerical optimization, we need to write a function that minimizes the distance between an arbitrary point on the plane and the given point $P$. This function is as follows:<br><br>\n",
        "$$min(f(x_1,x_2,x_3)) = min(\\sqrt{(x_1+1)^2+x_2^2+(x_3-1)^2})$$<br>\n",
        "$$s.t.\\text{     }g(x_1,x_2,x_3) = x_1+2x_2+3x_3-1$$<br>\n",
        "We can rewrite this as an unconstrained problem by rewriting $x_1$ in terms of $x_2,x_3$ to gurantee that the given point is always on the plane:<br><br>\n",
        "$$x_1 = -2x_2-3x_3+1$$<br>\n",
        "$${\\therefore}f(x_2,x_3) = \\sqrt{(-2x_2-3x_3+2)^2+x_2^2+(x_3-1)^2}$$\n",
        "Taking the gradient:\n",
        "$${\\nabla}f = \\left[\\begin{array}{cc}\n",
        "\\frac{-2(-2x_2-3x_3+2)+x_2}{\\sqrt{(-2x_2-3x_3+2)^2+x_2^2+(x_3-1)^2}} \\\\ \\frac{-3(-2x_2-3x_3+2)+(x_3-1)}{\\sqrt{(-2x_2-3x_3+2)^2+x_2^2+(x_3-1)^2}}\n",
        "\\end{array}\\right]$$\n",
        "\n",
        "(b) Gradient Descent:"
      ],
      "metadata": {
        "id": "ybNURSkoY67_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "stepsize = 0.1;\n",
        "Error = 0.000000001;\n",
        "maxiter = 1000;\n",
        "E = 1000;\n",
        "iter = 1;\n",
        "prev = [-0.143,0.7852];\n",
        "while E>=Error:\n",
        "  if iter>+maxiter:\n",
        "    break\n",
        "  x1 = 1 - 2*prev[0] - 3*prev[1];\n",
        "  gradient = [(-2*(x1+1)+prev[0])/(((x1+1)**2+(prev[1])**2+(prev[1]-1)**2)**0.5),(-3*(x1+1)+(prev[1]-1))/(((x1+1)**2+(prev[1])**2+(prev[1]-1)**2)**0.5)];\n",
        "  step = [element* -stepsize for element in gradient];\n",
        "  next = np.add(prev,step);\n",
        "  E = ((next[0]-prev[0])**2+(next[1]-prev[1])**2)**0.5\n",
        "  prev=next\n",
        "  iter = iter+1\n",
        "result = [-2*next[0]-3*next[1]+1,next[0],next[1]]\n",
        "print(result)\n",
        "print(f'Completed in {iter} iterations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBQo4O5WarVn",
        "outputId": "0b6159dc-f880-4be7-e8dc-cf9b4b64ac20"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.071428571428565, -0.14285713759030783, 0.7857142822030602]\n",
            "Completed in 79 iterations\n"
          ]
        }
      ]
    }
  ]
}